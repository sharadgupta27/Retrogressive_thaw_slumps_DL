#the parameter file for delineation of landforms using Deep Learning

##############################################################
## working folder and other directory setting

# working directory (current folder)
working_root = ./

##############################################################
## setting for getting training data

# get training data from study areas, for multiple areas, seperate them by using comma (,)
#training_regions = area_Willow_River.ini,area_Banks_east_nirGB.ini,area_Ellesmere_Island_nirGB.ini,area_Willow_River_nirGB.ini,area_Banks_east.ini,area_Ellesmere_Island.ini
training_regions = area_Willow_River.ini,area_Banks_east.ini

# the sub images for traing (relative path in the current folder)
# because somewhere in "datasets/get_subImages.py", we write "subImages" and "subLabels",
# we should not change values of input_train_dir and input_label_dir
input_train_dir= subImages
# the sub label images for training (relative path in the current folder)
input_label_dir= subLabels

# how many processes for getting sub-images, splitting sub-images and data augmentation
# don't set this too large because it has a lot IO operation
process_num = 8

#buffer size for extending the training polygon, in the XY projection, normally, it is based on meters
buffer_size = 300

#whether use the rectangular extent of the polygon, set "--rectangle" on right if Yes, or omit it if NO
b_use_rectangle = --rectangle

# for object detection, we don't need label images
#b_no_label_image = Yes

# ignore objects that touch the image in the label
b_ignore_edge_objects = Yes

#the nodata in output images, for sentinel, set dst_nodata as 0
dst_nodata = 0

# image format for splitting images: .tif or .png, or .jpg
split_image_format = .png

# the percentage of trainig data, the remaining are for validation
training_data_per = 0.9

# some pre-trained model was trained with 19 class (Cityscapes) or 21 classes (PASCAL VOC 2012), if yes, the model will try
# output 19 or 21 classes, but those classes without training data are nan.
b_initialize_last_layer = yes

# if the overall miou does not improve (< 0.0001) for five consecutive evaluation (epochs) on validation data, we stop training
b_early_stopping = yes

object_names = thaw_slump

# txt containing list for training and validation
# if set dataset_name as cityscapes, pascal_voc_seg, or ade20k, num_classes and ignore_label will use the one in 'data_generator.py'
dataset_name = thawslump_planet
training_sample_list_txt = train_list.txt
validation_sample_list_txt = val_list.txt

# image crop size (height, width) for the DeepLab model. Patch size should not greater than this
image_crop_size = 480,480

## patch width and height of training images (eg. 480=160+160*2)
train_patch_width = 160
train_patch_height = 160
train_pixel_overlay_x = 160
train_pixel_overlay_y = 160

# data augmentation
data_augmentation = blur,crop,bright,contrast,noise
# ignore class when perform data augmentation, multiple class will be support in future
data_aug_ignore_classes = class_0

# class number (without background)
NUM_CLASSES_noBG = 1

##############################################################
## deep learning setting
# experiment name
expr_name=exp1
# network setting files
network_setting_ini = yolov4_obj.cfg

##############################################################
## setting for inference (prediction)

# study areas for inference (prediction), for multiple areas, seperate them by using comma (,)
inference_regions = area_Willow_River.ini,area_Banks_east_nirGB.ini,area_Ellesmere_Island_nirGB.ini,area_Willow_River_nirGB.ini,area_Banks_east.ini,area_Ellesmere_Island.ini

# output folder for inference results
inf_output_dir = multi_inf_results

# indicate weather to use multiple available GPUs or only use one GPU (CPU)
b_use_multiGPUs = Yes

# the expected width of patch (70)
inf_patch_width= 160
# the expected height of patch (70)
inf_patch_height=160
# the overlay of patch in pixel (210)
inf_pixel_overlay_x=160
inf_pixel_overlay_y=160

# using python API when do the prediction
b_inf_use_python_api = Yes

# the batch size for prediction (only works when using python API)
inf_batch_size = 1


##############################################################
### Post processing and evaluation Parameters


# assuming ratio=height/width (suppose height > width), ratio belong to [0,1], if any polygon has ratio greater than
# maximum_ratio_width_height, it will be removed
maximum_ratio_width_height = 1.0

# the more narrow, the ratio (=perimeter^2/area) is larger
minimum_ratio_perimeter_area = 0

# the minimum mean elevation in meters (if dem_mean of a polygon small than this value, it will be removed)
minimum_elevation =

## only count the pixel within this range when do statistics
#dem_difference_range = None, -0.5
## expand the polygon when doing dem difference statistics (meters)
##buffer_size_dem_diff = 50

# the minimum area of total elevation reduction calculated by the two parameter above (m^2)
#minimum_dem_reduction_area = 4


# indicate whether to calculate the shape information (ratio_p_a, circularit, etc.)
b_calculate_shape_info = NO

IOU_threshold = 0.01

# if multiple observation available, remove polygons if at the same location, the occurrence of polygons is smaller than this threshold
threshold_occurrence_multi_observation = 1

# indicate whether remove false positive by utlizing multi-temporal mapping results (polygon-based analysis)
b_remove_polygons_using_multitemporal_results = NO

##############################################################
